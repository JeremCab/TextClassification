{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT with TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip # upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface-hub\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install -U transformers\n",
    "# !pip install -U datasets\n",
    "# !pip install datasets==1.18.1\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tqdm progress bars (on a terminal):\n",
    "1. `conda install -c conda-forge nodejs`\n",
    "2. `jupyter labextension install @jupyter-widgets/jupyterlab-manager`\n",
    "3. `jupyter nbextension enable --py widgetsnbextension`\n",
    "4. `jupyter lab clean`\n",
    "5. Refresh web page..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import transformers\n",
    "import datasets\n",
    "# import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current versions:\n",
      "1.0.2\n",
      "2.0.0\n",
      "4.18.0\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Current versions:\")\n",
    "print(sklearn.__version__)\n",
    "print(datasets.__version__)\n",
    "print(transformers.__version__)\n",
    "print(huggingface_hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import BertModel, DistilBertModel\n",
    "from transformers.data.data_collator import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device and Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "cache_dir = 'cache_dir/'\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "tfidf_dim = 4000\n",
    "\n",
    "alpha = 10\n",
    "learning_algo = RidgeClassifier(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions for loading and preparing data\n",
    "\n",
    "def tokenize(sample, tokenizer):\n",
    "    \"\"\"Tokenize sample\"\"\"\n",
    "    \n",
    "    sample = tokenizer(sample['text'], truncation=True, padding=False, return_length=True)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def load_and_tokenize_dataset(dataset_name, model_name='bert-base-uncased', cache_dir='cache_dir/'):\n",
    "    \"\"\"\n",
    "    Load dataset from the datasets library of HuggingFace.\n",
    "    Tokenize and sort data by length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "    \n",
    "    # Rename label column for tokenization purposes\n",
    "    dataset = dataset.rename_column('label', 'labels')\n",
    "    \n",
    "    # Tokenize data\n",
    "    dataset = dataset.map(lambda x: tokenize(x, tokenizer), batched=True)\n",
    "    \n",
    "    # sorting dataset\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].sort(\"length\")\n",
    "    \n",
    "    return dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to cache_dir/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe731004abf41479819419363fb14a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to cache_dir/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1704241155014c28a75860220028bdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0ce8fd130f41bc95b5a0da870caea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa492e9049d246128171864380a6a308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ce28b6713e463ca3e25d2534209a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset, tokenizer = load_and_tokenize_dataset('imdb', model_name=model_name, cache_dir=cache_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "indices: uint64\n",
       "----\n",
       "indices: [[10925,2404,17069,12114,373,15022,789,15780,5296,7220,...,12095,18867,17094,1684,1601,15321,11892,8640,11578,19874],[10372,18339,9262,22219,16904,20009,24892,1189,23709,1188,...,16722,22959,2211,13840,23806,12934,1425,9856,23301,1010],[3894,21201,12430,22905,1302,1650,5771,3547,22856,13274,...,12483,8582,15762,18222,16691,9367,2403,6859,11897,3453],[16090,18654,23247,15691,5158,22479,12175,19806,23350,4661,...,16577,7701,18931,18238,2870,2877,7704,22178,1914,14755],[19813,1917,17521,22238,2891,10764,19849,15684,10996,14890,...,9363,5505,10112,5170,439,17193,7605,7586,433,21236],[11381,13586,16184,15753,11275,23657,7494,5659,8923,17306,...,9049,21395,1942,6903,6547,14265,8508,7681,13967,16439],[14978,24282,9741,10598,9519,16016,18860,3046,24253,15841,...,1327,10533,9995,7558,3659,12020,2736,7198,23495,2667],[21509,17680,17844,21021,9365,20564,23848,251,16362,12383,...,5772,10264,5465,660,17515,22978,10954,13714,12649,1373],[722,1058,6797,19945,5786,13222,5884,21841,2759,18537,...,20183,21974,9349,10487,8687,8927,10660,19774,682,14293],[11459,8062,24314,1888,6100,12738,17186,24590,19683,22668,...,20204,9799,4544,10258,19607,538,18332,20756,6196,780],...[18290,4938,5549,10374,13165,15667,24115,792,1440,7117,...,2414,7053,8543,4149,13239,6492,6510,21651,20533,19488],[20742,13048,8289,5652,20619,23761,13656,2265,7089,21577,...,13574,22137,21131,6665,10781,22348,8024,24712,24810,22028],[7684,15059,19749,15909,5019,23203,19522,12749,15278,16621,...,7870,19409,2519,215,2624,5595,4808,6918,14966,3195],[7656,1635,2885,21013,23524,17963,16376,23798,14768,2536,...,2176,10658,5754,12551,20522,17213,4376,18031,11530,22243],[7553,21559,1984,24466,22250,3846,2733,20448,9150,4742,...,13764,13681,3313,19681,17058,13575,4303,11603,4530,13524],[11295,13516,6706,7317,1055,14753,12827,20590,21794,1652,...,3480,18081,15434,9642,23213,2740,14610,21269,21702,2927],[953,12069,8833,8864,294,13710,8181,2917,1163,11747,...,15837,15798,5012,165,5001,18227,4992,18102,180,19068],[15809,4966,4964,197,19065,198,15834,4954,23161,4953,...,9951,9933,9932,21276,13316,20064,21280,13318,9911,13326],[21284,9906,20061,13336,13314,20077,13353,9806,13452,20010,...,22553,22554,19381,6638,22557,6636,15015,19379,15017,15018],[6633,22549,22564,6689,14984,19431,14899,14913,14932,22496,...,14359,22126,14363,7801,7795,7794,19623,7785,22172,12499]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']._indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_features(dataset, dim=4000):\n",
    "    \"\"\"Compute tf-idf features and add it as a new field for the dataset\"\"\"\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=dim)\n",
    "    vectorizer.fit(dataset['train']['text'])\n",
    "        \n",
    "    for split in dataset.keys():\n",
    "        X_tmp = vectorizer.transform(dataset[split]['text'])\n",
    "        X_tmp = list(X_tmp.todense())\n",
    "        X_tmp = [np.asarray(row).reshape(-1) for row in X_tmp]\n",
    "        \n",
    "        indices = dataset[split]._indices # ***\n",
    "        dataset[split]._indices = None    # ***\n",
    "        dataset[split] = dataset[split].add_column(\"additional_fts\", X_tmp)\n",
    "        dataset[split]._indices = indices # ***\n",
    "        \n",
    "        dataset[split].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'length', 'additional_fts'])\n",
    "        dataset[split] = dataset[split].remove_columns(\"text\")\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    return dataset, t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, tfidf_time = get_tfidf_features(dataset, dim=tfidf_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length', 'additional_fts'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length', 'additional_fts'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length', 'additional_fts'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.95875430107117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 13,  15,  17,  ..., 512, 512, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['length'] # not sorted!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "indices: uint64\n",
       "----\n",
       "indices: [[10925,2404,17069,12114,373,15022,789,15780,5296,7220,...,12095,18867,17094,1684,1601,15321,11892,8640,11578,19874],[10372,18339,9262,22219,16904,20009,24892,1189,23709,1188,...,16722,22959,2211,13840,23806,12934,1425,9856,23301,1010],[3894,21201,12430,22905,1302,1650,5771,3547,22856,13274,...,12483,8582,15762,18222,16691,9367,2403,6859,11897,3453],[16090,18654,23247,15691,5158,22479,12175,19806,23350,4661,...,16577,7701,18931,18238,2870,2877,7704,22178,1914,14755],[19813,1917,17521,22238,2891,10764,19849,15684,10996,14890,...,9363,5505,10112,5170,439,17193,7605,7586,433,21236],[11381,13586,16184,15753,11275,23657,7494,5659,8923,17306,...,9049,21395,1942,6903,6547,14265,8508,7681,13967,16439],[14978,24282,9741,10598,9519,16016,18860,3046,24253,15841,...,1327,10533,9995,7558,3659,12020,2736,7198,23495,2667],[21509,17680,17844,21021,9365,20564,23848,251,16362,12383,...,5772,10264,5465,660,17515,22978,10954,13714,12649,1373],[722,1058,6797,19945,5786,13222,5884,21841,2759,18537,...,20183,21974,9349,10487,8687,8927,10660,19774,682,14293],[11459,8062,24314,1888,6100,12738,17186,24590,19683,22668,...,20204,9799,4544,10258,19607,538,18332,20756,6196,780],...[18290,4938,5549,10374,13165,15667,24115,792,1440,7117,...,2414,7053,8543,4149,13239,6492,6510,21651,20533,19488],[20742,13048,8289,5652,20619,23761,13656,2265,7089,21577,...,13574,22137,21131,6665,10781,22348,8024,24712,24810,22028],[7684,15059,19749,15909,5019,23203,19522,12749,15278,16621,...,7870,19409,2519,215,2624,5595,4808,6918,14966,3195],[7656,1635,2885,21013,23524,17963,16376,23798,14768,2536,...,2176,10658,5754,12551,20522,17213,4376,18031,11530,22243],[7553,21559,1984,24466,22250,3846,2733,20448,9150,4742,...,13764,13681,3313,19681,17058,13575,4303,11603,4530,13524],[11295,13516,6706,7317,1055,14753,12827,20590,21794,1652,...,3480,18081,15434,9642,23213,2740,14610,21269,21702,2927],[953,12069,8833,8864,294,13710,8181,2917,1163,11747,...,15837,15798,5012,165,5001,18227,4992,18102,180,19068],[15809,4966,4964,197,19065,198,15834,4954,23161,4953,...,9951,9933,9932,21276,13316,20064,21280,13318,9911,13326],[21284,9906,20061,13336,13314,20077,13353,9806,13452,20010,...,22553,22554,19381,6638,22557,6636,15015,19379,15017,15018],[6633,22549,22564,6689,14984,19431,14899,14913,14932,22496,...,14359,22126,14363,7801,7795,7794,19623,7785,22172,12499]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']._indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(dataset, tokenizer, batch_size=256):\n",
    "    dataloader_d = {}\n",
    "\n",
    "    for split in ['train', 'test']:\n",
    "        dataloader_d[split] = torch.utils.data.DataLoader(dataset[split], \n",
    "                                                          batch_size=batch_size, \n",
    "                                                          collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "        \n",
    "    return dataloader_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_d = create_dataloaders(dataset, tokenizer, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7fb9f07563d0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7fb9f0756a60>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in dataloader_d['train']:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- **Embedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements an embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name='bert-base-uncased', pooling='mean', device=torch.device('cpu')):\n",
    "        \n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name : str\n",
    "            Name of the BERT model and tokenizer.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        model_name : str\n",
    "            Name of the BERT model and tokenizer.\n",
    "            The list of or possible models is provided here: https://huggingface.co/models\n",
    "        pooling : str\n",
    "            Pooling strategy to be applied, either 'mean' or 'cls'.\n",
    "            For 'mean', the sentence embedding is the mean of the token embeddings.\n",
    "            For 'cls', the sentence embedding is the embedding of the [CSL] token (as usual in BERT).\n",
    "        device : torch.device\n",
    "            GPU is available, CPU otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.pooling = pooling\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = BertModel.from_pretrained(self.model_name, output_hidden_states=True)\n",
    "        self.model.to(self.device).eval()\n",
    "        print('Model downloaded:', model_name)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Embeds a batch of token ids into a 3D tensor.\n",
    "        If a GPU is available, the embedded batch is computed and put on the GPU.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: torch.Tensor\n",
    "            2D tensor: batch of text to be embedded.\n",
    "            Each sentence is represented as a vertical sequence of token ids.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        batch_emb : torch.Tensor\n",
    "            3D tensor (batch size x max sentence length x embedding dim)\n",
    "            BERT embedding of the batch of texts.\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            batch = batch.to(self.device)\n",
    "            \n",
    "            # DOES NOT IMPROVE THE RESULTS \n",
    "#             # New attention mask with last 1 element - correposnding to [SEP] token - removed.\n",
    "#             # Accordingly, the mean pooling will not take the embedding of [SEP] into account.\n",
    "#             last_indices = batch['length'] - 1\n",
    "#             batch_size = batch['length'].shape[0]\n",
    "#             indices = torch.tensor([range(batch_size), last_indices]).transpose(0,1)\n",
    "#             # cf. https://discuss.pytorch.org/t/modify-array-with-list-of-indices/27739\n",
    "#             batch['attention_mask'][indices[:, 0], indices[:, 1]] = 0\n",
    "            \n",
    "            if self.pooling == 'mean':\n",
    "                \n",
    "                batch_emb = self.model(batch[\"input_ids\"], batch[\"attention_mask\"])[0]\n",
    "                # batch_emb = torch.mean(batch_emb, dim=1)\n",
    "                # batch_emb = batch_emb.transpose(0, 1)\n",
    "                # batch_emb = batch_emb[:, :, :] # removing CLS and/or SEP does not seem to improve\n",
    "                batch_emb = torch.sum(batch_emb, dim=1).transpose(0, 1)\n",
    "                batch_emb = torch.div(batch_emb, batch['length']).transpose(0, 1)\n",
    "            \n",
    "            elif self.pooling == 'cls':\n",
    "            \n",
    "                batch_emb = self.model(batch[\"input_ids\"], batch[\"attention_mask\"])[1]\n",
    "\n",
    "            return batch_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "#embedding = Embedding()\n",
    "embedding = Embedding(pooling='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-base-uncased', 'mean', device(type='cuda'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.model_name, embedding.pooling, embedding.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = embedding(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **full model: embedding + feature concaatenation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTFIDF(nn.Module):\n",
    "    \"\"\"\n",
    "    Impdements BERT + TF-IDF model:\n",
    "    Concatenate BERT (or similar model) sentence embedding to most relevant TF-IDF features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', device=torch.device('cpu')):\n",
    "        \n",
    "        super(BertTFIDF, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.embedding = Embedding(model_name=self.model_name, device=self.device)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        embedded_input = self.embedding(batch)\n",
    "        additional_fts = batch['additional_fts']\n",
    "        \n",
    "        output = torch.cat([embedded_input, additional_fts], dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['additional_fts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "model = BertTFIDF(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, model, batch_size=256):\n",
    "    \"\"\"\n",
    "    Pass a dataset into a model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : datasets.arrow_dataset.Dataset\n",
    "        Dataset to be processed\n",
    "    model : __main__.BertTFIDF\n",
    "        Model instance of the BertTFIDF class\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    outputs_t, labels_t : torch.Tensor, torch.Tensor\n",
    "        Tuple of outputs and labels resulting from passing the dataset into the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                             batch_size=batch_size, \n",
    "                                             collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "    \n",
    "    outputs_t = torch.Tensor().to(device)\n",
    "    labels_t = torch.Tensor().to(device)\n",
    "\n",
    "    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch)\n",
    "        outputs_t = torch.cat([outputs_t, outputs], dim=0)\n",
    "\n",
    "        labels = batch['labels']\n",
    "        labels_t = torch.cat([labels_t, labels], dim=0)\n",
    "    \n",
    "    outputs_t = outputs_t.cpu().numpy()\n",
    "    labels_t = labels_t.cpu().numpy()\n",
    "    \n",
    "    return outputs_t, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783f7f3dcf614d3780de0a1c0a03f172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, y_train = process_dataset(dataset['train'], model, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_learning_algo(learning_algo, dataset, model, batch_size=256):\n",
    "    \"\"\"\n",
    "    Train the learning algorithm associated with the supervised pb (X_train, y_train).\n",
    "    More specifically, after the train set is passed through the model (EMB + POOL + ADD_TF-IDF), \n",
    "    a vector of X_train of text emeddings concatenated with TF-IDF features is obtained.\n",
    "    Then, the association between X_train and y_train is learned by means of a learning algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, y_train = process_dataset(dataset['train'], model, batch_size=batch_size)\n",
    "    \n",
    "    # fit sklearn learning algo\n",
    "    learning_algo.fit(X_train, y_train)\n",
    "    \n",
    "    return learning_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9ebc0cc6b547bcbed814eb60cce543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_algo = train_learning_algo(learning_algo, dataset, model, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(learning_algo, dataset, model, batch_size=256):\n",
    "    \"\"\"\n",
    "    Compute train and test predictions for the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, y_train = process_dataset(dataset['train'], model, batch_size=batch_size)\n",
    "    y_train_preds = learning_algo.predict(X_train)\n",
    "    \n",
    "    X_test, y_test = process_dataset(dataset['test'], model, batch_size=batch_size)\n",
    "    y_test_preds = learning_algo.predict(X_test)\n",
    "    \n",
    "    return y_train_preds, y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97daba24b1374150ac6323185b326b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced1f33eb3ec4716aa8c03e5f7221f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_preds, y_test_preds = predict(learning_algo, dataset, model, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39506/822105964.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(classification_report(y_test, y_test_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does reproduce the results!!!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0     0.9431    0.9582    0.9506     12500\n",
    "         1.0     0.9575    0.9422    0.9498     12500\n",
    "\n",
    "    accuracy                         0.9502     25000\n",
    "   macro avg     0.9503    0.9502    0.9502     25000\n",
    "weighted avg     0.9503    0.9502    0.9502     25000\n",
    "\n",
    "transformers==4.18.0 / 0.4.0 datasets==2.0.0 / batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After many experiments, the batch size seems to influence the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
