{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT with TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip # upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn\n",
    "# !pip install -U transformers\n",
    "# !pip install -U datasets\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tqdm progress bars (on a terminal):\n",
    "1. `conda install -c conda-forge nodejs`\n",
    "2. `jupyter labextension install @jupyter-widgets/jupyterlab-manager`\n",
    "3. `jupyter nbextension enable --py widgetsnbextension`\n",
    "4. `jupyter lab clean`\n",
    "5. Refresh web page..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check versions\n",
    "# import sklearn\n",
    "# import transformers\n",
    "# import datasets\n",
    "\n",
    "# print(\"Current versions:\")\n",
    "# print(sklearn.__version__)\n",
    "# print(datasets.__version__)\n",
    "# print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Current versions:\n",
    "1.0.2\n",
    "2.0.0\n",
    "4.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import src.model as mod\n",
    "from src.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device and seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# seeds (torch generator seed missing?)\n",
    "seed = 1979\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset folder & device\n",
    "dataset_name = 'trec' # 'trec'\n",
    "sort = True           # True\n",
    "\n",
    "# Model \n",
    "model_name = 'bert-base-uncased'\n",
    "tfidf_dim = 3000\n",
    "batch_size = 256\n",
    "pooling = 'mean'      # 'mean', 'mean_std', cls', 'mean_cls', 'mean_std_cls'.\n",
    "mode = 'default'      # 'default', 'bert_only', 'tfidf_only'\n",
    "\n",
    "# Learning algo\n",
    "alpha = 10\n",
    "\n",
    "# results\n",
    "results_folder = \"/raid/home/jeremiec/Data/TextClassification\"\n",
    "results_file = os.path.join(results_folder, dataset_name) + '.pkl'\n",
    "cache_dir = os.path.join(results_folder, 'cache_dir_' + dataset_name + '/')\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    os.system(\"rm -rf \" + cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tmp = torch.rand(size=(100000, 73500), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and tokenize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview of the datasets**\n",
    "1. Sentiment analysis\n",
    "    - ``IMDB``\n",
    "    - ``Yelp. P``\n",
    "    - ``Yelp. F``\n",
    "2. Question classification\n",
    "    - ``TREC``\n",
    "    - ``Yahoo! Answers``\n",
    "3. Topic detection\n",
    "    - ``AG News``\n",
    "    - ``DBPedia``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset trec/default (download: 350.79 KiB, generated: 403.39 KiB, post-processed: Unknown size, total: 754.18 KiB) to /raid/home/jeremiec/Data/TextClassification/cache_dir_trec/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aaef1f6cfa644468497a38b44fee1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ba5abdd36b403a8e424e1aad7fd0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/336k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fc4fa1c16449a3b542d8e8a32b7b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c199d9fe1641472e887927d3c5bb2a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d0ea40f678467e94f47ff2ac31aeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e905e700b4449daa52660d1eca48b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset trec downloaded and prepared to /raid/home/jeremiec/Data/TextClassification/cache_dir_trec/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af1a7d2d40549289b114015b38e3f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393a02a3d68e4cf8855fb888a82f2339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9956856825434acda8a37cd4e7614707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Sentiment\n",
    "# # IMDB\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('imdb', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "# # Yelp P. XXX CUDA OoM\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('yelp_polarity', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "# # Yelp F. XXX CUDA OoM\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('yelp_review_full', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "# # Question\n",
    "# # TREC\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('trec', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "# # Yahoo! Answers # XXX\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('yahoo_answers_topics', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "# # Topic\n",
    "# # AG NEWS\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('ag_news', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "# # DBPedia # XXX\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('dbpedia_14', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "\n",
    "dataset, tokenizer, model_name = load_and_tokenize_dataset(dataset_name, \n",
    "                                                           model_name=model_name, \n",
    "                                                           sort=sort,\n",
    "                                                           cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'label-fine', 'text', 'input_ids', 'token_type_ids', 'attention_mask', 'length'],\n",
       "        num_rows: 5452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'label-fine', 'text', 'input_ids', 'token_type_ids', 'attention_mask', 'length'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5452"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.939471753484959"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "81450 / 5452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].set_format(type='torch', columns=['input_ids', \n",
    "                                                 'attention_mask', \n",
    "                                                 'labels', \n",
    "                                                 'length'])\n",
    "\n",
    "# dataset['train'] = dataset['train'].remove_columns(text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset['train'],\n",
    "                                     # shuffle=False,\n",
    "                                     drop_last=False,\n",
    "                                     batch_size=batch_size,\n",
    "                                     collate_fn=DataCollatorWithPadding(tokenizer)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f11c0c701c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3658"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_pad = 0\n",
    "\n",
    "for b in dataloader:\n",
    "    nb_pad += (b['input_ids'] == 0).sum().item()\n",
    "\n",
    "nb_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features computed in 0.9909000396728516 sec.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "dataset = mod.get_tfidf_features(dataset, dim=tfidf_dim)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "tfidf_time = t1 - t0\n",
    "print(f\"Features computed in {tfidf_time} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "model = mod.BertTFIDF(model_name=model_name, pooling=pooling, mode=mode, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856402be55bd4d4dacdccfacc8f12afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Processing data\n",
    "t0 = time.time()\n",
    "\n",
    "X_train, y_train = process_dataset(dataset['train'], model, tokenizer, device, batch_size)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "training_time = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "# Note: if different alpha are tested, insert a loop here\n",
    "t0 = time.time()\n",
    "\n",
    "learning_algo = RidgeClassifier(alpha=alpha)\n",
    "learning_algo.fit(X_train, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "fitting_time = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD VERSION\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# learning_algo = train_learning_algo(learning_algo, dataset, model, tokenizer, \n",
    "#                                     device, batch_size)\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# training_time = t1 - t0\n",
    "# print(print(f\"Model trained in {training_time} sec.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428e23e51c9e48bfbb9de830d0803f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test, y_test_preds = predict(learning_algo, dataset, model, tokenizer, \n",
    "                               device, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9259    0.9058    0.9158       138\n",
      "         1.0     0.8272    0.7128    0.7657        94\n",
      "         2.0     1.0000    0.7778    0.8750         9\n",
      "         3.0     0.8857    0.9538    0.9185        65\n",
      "         4.0     0.8819    0.9912    0.9333       113\n",
      "         5.0     0.9125    0.9012    0.9068        81\n",
      "\n",
      "    accuracy                         0.8920       500\n",
      "   macro avg     0.9055    0.8738    0.8859       500\n",
      "weighted avg     0.8913    0.8920    0.8897       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "test_results = classification_report(y_test, y_test_preds, digits=4, output_dict=True)\n",
    "print(classification_report(y_test, y_test_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0': {'precision': 0.9259259259259259,\n",
       "  'recall': 0.9057971014492754,\n",
       "  'f1-score': 0.9157509157509157,\n",
       "  'support': 138},\n",
       " '1.0': {'precision': 0.8271604938271605,\n",
       "  'recall': 0.7127659574468085,\n",
       "  'f1-score': 0.7657142857142857,\n",
       "  'support': 94},\n",
       " '2.0': {'precision': 1.0,\n",
       "  'recall': 0.7777777777777778,\n",
       "  'f1-score': 0.8750000000000001,\n",
       "  'support': 9},\n",
       " '3.0': {'precision': 0.8857142857142857,\n",
       "  'recall': 0.9538461538461539,\n",
       "  'f1-score': 0.9185185185185185,\n",
       "  'support': 65},\n",
       " '4.0': {'precision': 0.8818897637795275,\n",
       "  'recall': 0.9911504424778761,\n",
       "  'f1-score': 0.9333333333333333,\n",
       "  'support': 113},\n",
       " '5.0': {'precision': 0.9125,\n",
       "  'recall': 0.9012345679012346,\n",
       "  'f1-score': 0.9068322981366459,\n",
       "  'support': 81},\n",
       " 'accuracy': 0.892,\n",
       " 'macro avg': {'precision': 0.9055317448744832,\n",
       "  'recall': 0.8737620001498545,\n",
       "  'f1-score': 0.8858582252422832,\n",
       "  'support': 500},\n",
       " 'weighted avg': {'precision': 0.8913366721520922,\n",
       "  'recall': 0.892,\n",
       "  'f1-score': 0.8896991115004157,\n",
       "  'support': 500}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 7.449293613433838 sec.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training time: {tfidf_time + training_time + fitting_time} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dataset, tokenizer, model, learning_algo\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (651886343.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1912/651886343.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    XXX stop here\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "XXX stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as fh:\n",
    "        results_d = pickle.load(fh)\n",
    "else:\n",
    "    results_d = {}\n",
    "\n",
    "    \n",
    "key = (pooling, mode, tfidf_dim, alpha, batch_size)\n",
    "results_d[key] = (test_results, \n",
    "                  tfidf_time + training_time + fitting_time, \n",
    "                  \"pooling - mode - tfidf_dim - alpha - batch_size\")\n",
    "\n",
    "with open(results_file, 'wb') as fh:\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**\n",
    "\n",
    "- We chose to implement the **standard mean:** ``torch.mean(batch, dim=1)``: <br>\n",
    "    Instead of a custom mean where we sum the embedded tokens and divide by the sentence length, we implement a standard mean of the batch. After some experiments, this seems to work better.\n",
    "- Similarly, we chose to implement the **standard std:** ``torch.std(batch, dim=1)``: <br>\n",
    "    Instead of a custom std involving the sentence length, we implement a standard std of the batch. After some experiments, this seems to work better.\n",
    "- The **sorting by length** process infuences the results: <br>\n",
    "For some datasets, the results are better when the data are sorted by length, while for others, the opposite holds true. This phenomenon is probably due to the mean operation applied to the batch, which would yield very different results depending on whether the batch is sorted or not.\n",
    "- The **batch size** singificantly influences the results: <br>\n",
    "    Large batches include more padding than small batches. These padded tokens are actually involved in the computation of the mean, and thus influences it. For this reason, the batch size influences the results. The batch size will be a hyperparameter of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
