{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT with TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip # upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn\n",
    "# !pip install -U transformers\n",
    "# !pip install -U datasets\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tqdm progress bars (on a terminal):\n",
    "1. `conda install -c conda-forge nodejs`\n",
    "2. `jupyter labextension install @jupyter-widgets/jupyterlab-manager`\n",
    "3. `jupyter nbextension enable --py widgetsnbextension`\n",
    "4. `jupyter lab clean`\n",
    "5. Refresh web page..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check versions\n",
    "# import sklearn\n",
    "# import transformers\n",
    "# import datasets\n",
    "# \n",
    "# print(\"Current versions:\")\n",
    "# print(sklearn.__version__)\n",
    "# print(datasets.__version__)\n",
    "# print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Current versions:\n",
    "1.0.2\n",
    "2.0.0\n",
    "4.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import src.model as mod\n",
    "from src.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device and seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# seeds (torch generator seed missing?)\n",
    "seed = 1979\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset folder & device\n",
    "dataset_name = 'imdb' # 'trec'\n",
    "sort = True          # True\n",
    "\n",
    "# Model \n",
    "model_name = 'bert-base-uncased'\n",
    "tfidf_dim = 3000\n",
    "batch_size = 128\n",
    "pooling = 'mean'      # 'mean', 'mean_std', cls', 'mean_cls', 'mean_std_cls'.\n",
    "mode = 'default'      # 'default', 'bert_only', 'tfidf_only'\n",
    "\n",
    "# Learning algo\n",
    "alpha = 10\n",
    "\n",
    "# results\n",
    "results_folder = \"/raid/home/jeremiec/Data/TextClassification\"\n",
    "results_file = os.path.join(results_folder, dataset_name) + '.pkl'\n",
    "cache_dir = os.path.join(results_folder, 'cache_dir_' + dataset_name + '/')\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    os.system(\"rm -rf \" + cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tmp = torch.rand(size=(100000, 73500), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and tokenize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview of the datasets**\n",
    "1. Sentiment analysis\n",
    "    - ``IMDB``\n",
    "    - ``Yelp. P``\n",
    "    - ``Yelp. F``\n",
    "2. Question classification\n",
    "    - ``TREC``\n",
    "    - ``Yahoo! Answers``\n",
    "3. Topic detection\n",
    "    - ``AG News``\n",
    "    - ``DBPedia``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /raid/home/jeremiec/Data/TextClassification/cache_dir_imdb/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59f39b119ec4e5987bb55d68e207c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /raid/home/jeremiec/Data/TextClassification/cache_dir_imdb/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1840607e51451f9cf5cdee7762ff7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31de26179d7401398e9850bb86f9614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09339374e96497aba643573e8a49208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27fa96f5c6247bb9740cc03a3725e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Sentiment\n",
    "# # IMDB\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('imdb', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "# # Yelp P. XXX CUDA OoM\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('yelp_polarity', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "# # Yelp F. XXX CUDA OoM\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('yelp_review_full', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "# # Question\n",
    "# # TREC\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('trec', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "# # Yahoo! Answers # XXX\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('yahoo_answers_topics', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "# # Topic\n",
    "# # AG NEWS\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('ag_news', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "# # DBPedia # XXX\n",
    "# dataset, tokenizer, model_name = load_and_tokenize_dataset('dbpedia_14', model_name=model_name, cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "\n",
    "dataset, tokenizer, model_name = load_and_tokenize_dataset(dataset_name, \n",
    "                                                           model_name=model_name, \n",
    "                                                           sort=sort,\n",
    "                                                           cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'length'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features computed in 33.72184348106384 sec.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "dataset = mod.get_tfidf_features(dataset, dim=tfidf_dim)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "tfidf_time = t1 - t0\n",
    "print(f\"Features computed in {tfidf_time} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "model = mod.BertTFIDF(model_name=model_name, pooling=pooling, mode=mode, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae48f714951a4034845ed9d4975343de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Processing data\n",
    "t0 = time.time()\n",
    "\n",
    "X_train, y_train = process_dataset(dataset['train'], model, tokenizer, device, batch_size)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "training_time = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "# Note: if different alpha are tested, insert a loop here\n",
    "t0 = time.time()\n",
    "\n",
    "learning_algo = RidgeClassifier(alpha=alpha)\n",
    "learning_algo.fit(X_train, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "fitting_time = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD VERSION\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# learning_algo = train_learning_algo(learning_algo, dataset, model, tokenizer, \n",
    "#                                     device, batch_size)\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# training_time = t1 - t0\n",
    "# print(print(f\"Model trained in {training_time} sec.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db335b6444c4c8b87eb0d4b947867bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test, y_test_preds = predict(learning_algo, dataset, model, tokenizer, \n",
    "                               device, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9446    0.9582    0.9513     12500\n",
      "         1.0     0.9576    0.9438    0.9506     12500\n",
      "\n",
      "    accuracy                         0.9510     25000\n",
      "   macro avg     0.9511    0.9510    0.9510     25000\n",
      "weighted avg     0.9511    0.9510    0.9510     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "test_results = classification_report(y_test, y_test_preds, digits=4, output_dict=True)\n",
    "print(classification_report(y_test, y_test_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0': {'precision': 0.9446328574808739,\n",
       "  'recall': 0.95816,\n",
       "  'f1-score': 0.9513483458437587,\n",
       "  'support': 12500},\n",
       " '1.0': {'precision': 0.9575521467413359,\n",
       "  'recall': 0.94384,\n",
       "  'f1-score': 0.9506466298698683,\n",
       "  'support': 12500},\n",
       " 'accuracy': 0.951,\n",
       " 'macro avg': {'precision': 0.951092502111105,\n",
       "  'recall': 0.9510000000000001,\n",
       "  'f1-score': 0.9509974878568135,\n",
       "  'support': 25000},\n",
       " 'weighted avg': {'precision': 0.9510925021111049,\n",
       "  'recall': 0.951,\n",
       "  'f1-score': 0.9509974878568135,\n",
       "  'support': 25000}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 163.60133409500122 sec.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training time: {tfidf_time + training_time + fitting_time} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dataset, tokenizer, model, learning_algo\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'rb') as fh:\n",
    "        results_d = pickle.load(fh)\n",
    "else:\n",
    "    results_d = {}\n",
    "\n",
    "    \n",
    "key = (pooling, mode, tfidf_dim, alpha, batch_size)\n",
    "results_d[key] = (test_results, \n",
    "                  tfidf_time + training_time + fitting_time, \n",
    "                  \"pooling - mode - tfidf_dim - alpha - batch_size\")\n",
    "\n",
    "with open(results_file, 'wb') as fh:\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('mean',\n",
       "  'default',\n",
       "  3000,\n",
       "  10,\n",
       "  32): ({'0.0': {'precision': 0.900877893056664,\n",
       "    'recall': 0.90304,\n",
       "    'f1-score': 0.9019576508190172,\n",
       "    'support': 12500},\n",
       "   '1.0': {'precision': 0.9028067361668003,\n",
       "    'recall': 0.90064,\n",
       "    'f1-score': 0.9017220664797757,\n",
       "    'support': 12500},\n",
       "   'accuracy': 0.90184,\n",
       "   'macro avg': {'precision': 0.9018423146117321,\n",
       "    'recall': 0.90184,\n",
       "    'f1-score': 0.9018398586493965,\n",
       "    'support': 25000},\n",
       "   'weighted avg': {'precision': 0.9018423146117321,\n",
       "    'recall': 0.90184,\n",
       "    'f1-score': 0.9018398586493965,\n",
       "    'support': 25000}}, 291.1757595539093, 'pooling - mode - tfidf_dim - alpha - batch_size'),\n",
       " ('mean',\n",
       "  'default',\n",
       "  3000,\n",
       "  10,\n",
       "  128): ({'0.0': {'precision': 0.9446328574808739,\n",
       "    'recall': 0.95816,\n",
       "    'f1-score': 0.9513483458437587,\n",
       "    'support': 12500},\n",
       "   '1.0': {'precision': 0.9575521467413359,\n",
       "    'recall': 0.94384,\n",
       "    'f1-score': 0.9506466298698683,\n",
       "    'support': 12500},\n",
       "   'accuracy': 0.951,\n",
       "   'macro avg': {'precision': 0.951092502111105,\n",
       "    'recall': 0.9510000000000001,\n",
       "    'f1-score': 0.9509974878568135,\n",
       "    'support': 25000},\n",
       "   'weighted avg': {'precision': 0.9510925021111049,\n",
       "    'recall': 0.951,\n",
       "    'f1-score': 0.9509974878568135,\n",
       "    'support': 25000}}, 163.60133409500122, 'pooling - mode - tfidf_dim - alpha - batch_size')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**\n",
    "\n",
    "- We chose to implement the **standard mean:** ``torch.mean(batch, dim=1)``: <br>\n",
    "    Instead of a custom mean where we sum the embedded tokens and divide by the sentence length, we implement a standard mean of the batch. After some experiments, this seems to work better.\n",
    "- Similarly, we chose to implement the **standard std:** ``torch.std(batch, dim=1)``: <br>\n",
    "    Instead of a custom std involving the sentence length, we implement a standard std of the batch. After some experiments, this seems to work better.\n",
    "- The **sorting by length** process infuences the results: <br>\n",
    "For some datasets, the results are better when the data are sorted by length, while for others, the opposite holds true. This phenomenon is probably due to the mean operation applied to the batch, which would yield very different results depending on whether the batch is sorted or not.\n",
    "- The **batch size** singificantly influences the results: <br>\n",
    "    Large batches include more padding than small batches. These padded tokens are actually involved in the computation of the mean, and thus influences it. For this reason, the batch size influences the results. The batch size will be a hyperparameter of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
